<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.3">Jekyll</generator><link href="https://anique.ml/feed.xml" rel="self" type="application/atom+xml" /><link href="https://anique.ml/" rel="alternate" type="text/html" hreflang="en" /><updated>2024-04-20T16:19:50+00:00</updated><id>https://anique.ml/feed.xml</id><title type="html">blank</title><subtitle>Anique&apos;s Website.
</subtitle><entry><title type="html">Debugging a Machine Learning Architecture - A Step-by-Step Guide</title><link href="https://anique.ml/blog/2024/model_debugging/" rel="alternate" type="text/html" title="Debugging a Machine Learning Architecture - A Step-by-Step Guide" /><published>2024-03-30T15:59:00+00:00</published><updated>2024-03-30T15:59:00+00:00</updated><id>https://anique.ml/blog/2024/model_debugging</id><content type="html" xml:base="https://anique.ml/blog/2024/model_debugging/"><![CDATA[<p>Machine learning (ML) projects are inherently complex and multifaceted, often involving intricate architectures and numerous data processing steps. When an ML model doesn’t perform as expected, debugging becomes a crucial skill to identify and fix the issues. In this blog post, I’ll walk you through a comprehensive approach to debugging a machine learning architecture, using a hypothetical sentiment analysis model as an example. This model aims to classify text inputs into positive, negative, or neutral sentiments.</p>
<h2 id="step-1-problem-definition-and-understanding">Step 1: Problem Definition and Understanding</h2>

<p>Before diving into debugging, ensure you clearly understand the problem you’re solving and the expected behavior of your model. For our sentiment analysis model, define the scope of sentiments you’re classifying and the granularity of sentiment you aim to detect.</p>
<h2 id="step-2-sanity-checks">Step 2: Sanity Checks</h2>

<p>Start with basic sanity checks:</p>

<p>Data Integrity: Verify your data is correctly loaded and formatted. Check for missing values, unexpected characters, or encoding issues in your dataset.
Model Compilation: Ensure your model compiles without errors. This includes checking the model architecture, loss functions, and optimizer settings.
Overfitting on a Small Dataset: Try overfitting your model on a small subset of the data. If the model can’t overfit a small dataset, there might be issues with the model architecture or the data processing steps.</p>

<h2 id="step-3-data-processing-and-feature-engineering">Step 3: Data Processing and Feature Engineering</h2>

<p>Errors in data preprocessing or feature engineering can significantly impact model performance:</p>

<p>Data Normalization: Confirm that all numerical features are normalized or standardized appropriately.
Feature Selection: Assess whether the features used are relevant and sufficient for the model to learn the task.
Data Augmentation: If you’re using data augmentation, ensure that the augmentations are correctly applied and don’t introduce noise or irrelevant variations.</p>

<h2 id="step-4-model-architecture">Step 4: Model Architecture</h2>

<p>The model architecture must be suitable for the task:</p>

<p>Layer Configuration: Check if the layers and their parameters (e.g., number of units in a dense layer, filter sizes in convolutional layers) are appropriate for your problem.
Activation Functions: Ensure you’re using suitable activation functions. Incorrect functions (e.g., using a sigmoid function for a multi-class classification problem) can hinder model learning.
Regularization: If your model is overfitting, consider adding regularization methods like dropout, L1/L2 regularization, or using a simpler model architecture.</p>

<h2 id="step-5-training-process">Step 5: Training Process</h2>

<p>The training process is crucial for model convergence:</p>

<p>Learning Rate: A too high or too low learning rate can cause the model to diverge or converge too slowly. Use learning rate schedules or find an optimal learning rate empirically.
Batch Size: Adjust the batch size if necessary. Small batches can offer more robust convergence at the cost of training stability, while large batches may be more stable but potentially less effective at finding the global minimum.
Epochs and Early Stopping: Ensure you’re training the model for an adequate number of epochs. Implement early stopping to prevent overfitting.</p>

<h2 id="step-6-evaluation-and-metrics">Step 6: Evaluation and Metrics</h2>

<p>Choosing the right evaluation metrics is vital for assessing model performance:</p>

<p>Metric Selection: Use metrics that align with your problem’s goals. For sentiment analysis, accuracy, precision, recall, and F1-score might be relevant.
Validation Set Performance: Monitor performance on a validation set to gauge generalization. A significant performance gap between training and validation sets indicates overfitting.</p>

<h2 id="step-7-iterative-improvement">Step 7: Iterative Improvement</h2>

<p>Debugging is an iterative process:</p>

<p>Error Analysis: Analyze the types of errors your model is making. Are there particular classes or types of data it struggles with?
Model Adjustments: Based on error analysis, adjust your model. This could involve collecting more data for underrepresented classes, tweaking the architecture, or revisiting feature engineering.</p>

<h2 id="conclusion">Conclusion</h2>

<p>Debugging a machine learning model is a systematic process that requires patience and careful analysis at each step. By methodically working through each component of your ML architecture—from data preprocessing to model evaluation—you can identify bottlenecks and issues that affect performance. Remember, debugging is not just about fixing problems; it’s also an opportunity to understand your model and the problem it’s solving on a deeper level. Through this meticulous approach, you’ll enhance your model’s performance and, ultimately, its ability to solve the problem at hand.</p>]]></content><author><name></name></author><category term="machine-learning" /><summary type="html"><![CDATA[a short guide on making sure everything works out for your ML solution]]></summary></entry><entry><title type="html">Llama-2 finetuning using PEFT and Multi-GPU setup</title><link href="https://anique.ml/blog/2023/jax_lora/" rel="alternate" type="text/html" title="Llama-2 finetuning using PEFT and Multi-GPU setup" /><published>2023-12-06T15:59:00+00:00</published><updated>2023-12-06T15:59:00+00:00</updated><id>https://anique.ml/blog/2023/jax_lora</id><content type="html" xml:base="https://anique.ml/blog/2023/jax_lora/"><![CDATA[<p>ChatGPT and Gemini are great models for general prompting. However, due to model alignment, they refuse to respond to prompts related to research which involves toxic language. Llama-2 provides an open-source alternative to train an unaligned model. Thus, for one of my recent research, we needed to fine-tune a Llama-2 model. For the larger models, I also needed multi-gpu setup to fit the model in memory for training, especially due to large context sizes. However, I soon realized that open-source implementations of LoRA training implementations are limited. Multi-GPU inference on the other hand is as simple as using <code class="language-plaintext highlighter-rouge">auto</code> for the device mapping in the hugging face implementation.</p>

<p>“There’s two strategies that have been shown to work: Gpipe-style model parallelism, and tensor parallelism. HF Accelerate and Deepspeed both support the former. However sadly they don’t properly support LoRA at present.” -<a href="https://www.reddit.com/r/LocalLLaMA/comments/166h6bx/model_parallelism_with_lora/jyk5q6j/">Jeremy Howard</a></p>

<p>In addition to the limitations of accelerate and deepspeed, I also tried #anyscale’s ray/alpa to further disappointment. Perhaps thing’s have changed recently since I started working on this. However, with my solution, I never needed to look back.</p>

<p>I have always found Jax to be convenient to work with and much faster than pytorch, so I implemented LoRA and GPU tensor parallelism on top of <a href="https://github.com/ayaka14732/llama-2-jax">ayaka14732</a>’s excellent implementation of Llama-2 on JAX. In addition, I change the codebase to implement the following changes:</p>
<ul>
  <li>Alpaca format instruct dataset loading</li>
  <li>Parameter configuration for 13B models</li>
  <li>Hand-tuned sharding for projection and attention parameters for optimized distribution of parameters across GPU’s</li>
  <li>Merging of parameters to huggingface after training (for using other huggingface compatible libraries)</li>
  <li>python 3.9 support</li>
</ul>

<p>Update 03/29/2024
Several Quality of Life changes have been made to my codebase. I also published a preprint for my library. Evaluation of my approach shows over 12x performance improvement over HuggingFace PEFT/Microsoft DeepSpeed.</p>

<p><a href="https://arxiv.org/abs/2403.11366">Preprint</a>
<a href="https://github.com/aniquetahir/jora">Code</a></p>

<p>#machinelearning #generativeai</p>]]></content><author><name></name></author><category term="machine-learning" /><category term="jax" /><category term="generative-ai" /><summary type="html"><![CDATA[repository for the implementation of PEFT finetuning using jax]]></summary></entry><entry><title type="html">training large language models using distributed resources</title><link href="https://anique.ml/blog/2023/distributed_llms/" rel="alternate" type="text/html" title="training large language models using distributed resources" /><published>2023-07-27T00:00:00+00:00</published><updated>2023-07-27T00:00:00+00:00</updated><id>https://anique.ml/blog/2023/distributed_llms</id><content type="html" xml:base="https://anique.ml/blog/2023/distributed_llms/"><![CDATA[<h2 id="why-do-we-need-distributed-training">Why do we need Distributed Training?</h2>
<p>Large language models such as OpenAI’s ChatGPT and Meta’s Llama have gained popularity due to their usefulness. One of the major criticism of ChatGPT is that it’s training parameters are ironically closed source. However, Meta’s Llama models provide an Open Source alternative albeit being behind in quality. The NLP community has leveraged Llama to create variations which are intended to improve on the original model. Some improvements include uncensoring the model so that it is capable of responding in an unmoderated manner or finetuning it to provide better responses to instruct prompts. The possibilities of using open source LLMs are limitless. One simple application is to fine-tune Llama on a specific dataset. However, due to the size of the model, some of the larger versions may not fit on a single GPU. Hence, it is vital to learn how to train these models over a number of GPUs (multi-gpu) or a number of nodes each consisting of several GPUs (multi-node).</p>

<h2 id="basics-of-distributed-training">Basics of Distributed Training</h2>
<p>In this post, I will focus on distributed training over a single node containing multiple GPUs. One does not need to train models on a GPU. In fact, if one were interested in using the cloud, TPUs are a promising alternative. However, TPUs are not sold to the public and I am a promoted of doing things on hardware which one has physical access to. Thus, I will focus on training on NVidia GPUs (the other alternative, AMD, does not have good support for machine learning as per current date). This post will be hands-on with code examples you can run.</p>

<p>The first step is to check whether your rig has multiple GPUs and how they are linked together. The following command will return the status of the GPUs including the memory consumption and the processing power being consumed. It is handy to run this utility during training to get an idea of the resources being consumed (or not being consumed). In case the processing power is not 100%, it is indicative of a bottleneck in the training script which needs to be fixed.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvidia-smi
</code></pre></div></div>

<p>Next, the following command helps to see how the GPUs are linked together:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvidia-smi topo -m
</code></pre></div></div>
<p>Output:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	GPU0	GPU1	CPU Affinity	NUMA Affinity	GPU NUMA ID
GPU0	 X 	PHB	0-11		N/A		N/A
GPU1	PHB	 X 	0-11		N/A		N/A

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks
</code></pre></div></div>

<p>NVidia cards allow being linked together using NVlink cables. This helps cross-GPU communication. However, as visible from the output, the cards need not be linked in such a way. Even if the GPUs are connected to seperate PCI ports on the same node. They can still communicate. However, cross-GPU communication will be slower. When designing a model for distributed training, the topology of the resources needs to be taken into consideration.</p>

<p>There are multiple ways to parallelize a model e.g. Data Parallelism, Pipeline Parallelism, etc. There are pros and cons of each strategy and they can be mixed together.</p>

<h2 id="limitations-of-current-implementations">Limitations of current implementations</h2>

<hr />
<p>To be continued
(This post is a Work in Progress)</p>]]></content><author><name>Anique Tahir</name></author><summary type="html"><![CDATA[With the increasing popularity of large language models, it becomes increasingly important to use all available resources to gain an edge in training.]]></summary></entry><entry><title type="html">a post with giscus comments</title><link href="https://anique.ml/blog/2022/giscus-comments/" rel="alternate" type="text/html" title="a post with giscus comments" /><published>2022-12-10T15:59:00+00:00</published><updated>2022-12-10T15:59:00+00:00</updated><id>https://anique.ml/blog/2022/giscus-comments</id><content type="html" xml:base="https://anique.ml/blog/2022/giscus-comments/"><![CDATA[<p>This post shows how to add GISCUS comments.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="external-services" /><summary type="html"><![CDATA[an example of a blog post with giscus comments]]></summary></entry><entry><title type="html">understanding jax’s scan function</title><link href="https://anique.ml/blog/2022/understanding_scan/" rel="alternate" type="text/html" title="understanding jax’s scan function" /><published>2022-09-05T00:00:00+00:00</published><updated>2022-09-05T00:00:00+00:00</updated><id>https://anique.ml/blog/2022/understanding_scan</id><content type="html" xml:base="https://anique.ml/blog/2022/understanding_scan/"><![CDATA[<h2 id="why-use-jaxlaxscan">Why use <code class="language-plaintext highlighter-rouge">jax.lax.scan</code></h2>

<p>Jax is a neural network library used mostly by Google. Jax converts all your implementation into a graph which is executed
on your CPU, GPU or TPU. There are two main advantages of using Jax for your implementation:</p>
<ul>
  <li>It is comparatively faster than it’s competitor Pytorch</li>
  <li>It allows for significantly easier implementation for low level neural network concepts (albeit being harder for high-level</li>
  <li>ideas)</li>
</ul>

<p>Jax allows you to <code class="language-plaintext highlighter-rouge">jit</code> your functions. <code class="language-plaintext highlighter-rouge">jit</code> stands for Just-In-Time compilation. This makes your function significantly fast
since it is compiled into something native to the GPU<d-footnote>or any device you compile for.</d-footnote>. However, the drawback 
is that the amount of memory that your function will use, has to be pre-specified. This means that functions containing loops
have to be changed, since the length of the loop can be arbitrary. <code class="language-plaintext highlighter-rouge">jax.lax.scan</code> allows you to get around this limitation by
allowing you to define a loop with pre-specified length. But how does it work?</p>

<hr />

<p>According to the jax <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html">documentation</a>, the following code is 
essentially a translation of the function in pythonic form:</p>

<pre><code class="language-python3">def scan(f, init, xs, length=None):
  if xs is None:
    xs = [None] * length
  carry = init
  ys = []
  for x in xs:
    carry, y = f(carry, x)
    ys.append(y)
  return carry, np.stack(ys)
</code></pre>

<p>The code may be a little convoluted to understand. A simpler way to understand it is to look at some simple examples. The <code class="language-plaintext highlighter-rouge">scan</code> 
function takes three parameters and scans over the third argument. The first arguments is a function to execute over each scan iteration.
The second argument is some <code class="language-plaintext highlighter-rouge">pytree</code> structure which we initially start from. Lets look at a simple example.</p>

<pre><code class="language-python3">scan(lambda x, y: (x+y, y+2,), 0, [1, 2, 3])
</code></pre>

<p>The above code essentially scans through the list <code class="language-plaintext highlighter-rouge">[1, 2, 3]</code> and for each element, returns a tuple consisting of the previous element and the current element. The output of the above code will be:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</code></pre></div></div>
<p>Here <code class="language-plaintext highlighter-rouge">x</code> is the carry argument which is initialized to <code class="language-plaintext highlighter-rouge">0</code>. <code class="language-plaintext highlighter-rouge">y</code> is each element of the array <code class="language-plaintext highlighter-rouge">[1, 2, 3]</code> passed sequentially to the function. In the first pass <code class="language-plaintext highlighter-rouge">x</code> is <code class="language-plaintext highlighter-rouge">0</code> and <code class="language-plaintext highlighter-rouge">y</code> is 1. The return value is <code class="language-plaintext highlighter-rouge">(x+y, y+2,) = (0+1,  1+2,) = (1, 3,)</code>. For the next iteration, <code class="language-plaintext highlighter-rouge">x</code> is <code class="language-plaintext highlighter-rouge">1</code>, since <code class="language-plaintext highlighter-rouge">1</code> is the value of the carry returned in the last iteration and <code class="language-plaintext highlighter-rouge">y</code> is <code class="language-plaintext highlighter-rouge">2</code>, since <code class="language-plaintext highlighter-rouge">2</code> is the next value in the input array. Thus, <code class="language-plaintext highlighter-rouge">(x+y, y+2,) = (1+2,  2+2,) = (3, 4,)</code>. In the next iteration, the final value of the carry is <code class="language-plaintext highlighter-rouge">6</code>, while the final <code class="language-plaintext highlighter-rouge">y+2</code> is <code class="language-plaintext highlighter-rouge">5</code>. Thus, the <code class="language-plaintext highlighter-rouge">scan</code> function returns <code class="language-plaintext highlighter-rouge">6</code>(the carry) and <code class="language-plaintext highlighter-rouge">[3, 4, 5]</code>(all the <code class="language-plaintext highlighter-rouge">y+2</code> concatenated)</p>

<p>Here we used a simple example, but the scan function can be used over more complicated arguments, such as the training loop of a neural network. This makes it possible to jit compile the entire training process, resulting in a large gain in training speed.</p>

<h2 id="useful-notes-on-scan-behaviour">Useful notes on scan behaviour</h2>
<p>In jax, it is common to use <code class="language-plaintext highlighter-rouge">NamedTuple</code>s to store various things such as model parameters. The third argument of scan can be used to iterate over the batch dimension of elements in the <code class="language-plaintext highlighter-rouge">NamedTuple</code>. Here is an example. Consider a transformer decoder with 32 blocks (Llama). Parameter values may be saved in a single <code class="language-plaintext highlighter-rouge">NamedTuple</code> as follows:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DecoderBlock(
  input_norm=(32, 4096),
  attention=Attention(
    q_proj=(32, 4096, 1, 32, 128),
    k_proj=(32, 4096, 32, 128),
    v_proj=(32, 4096, 32, 128),
    out_proj=(32, 1, 32, 128, 4096)
  ),
  post_attn_norm=(32, 4096), gate_proj=(32, 4096, 11008),
  up_proj=(32, 4096, 11008), down_proj=(32, 11008, 4096)
)
</code></pre></div></div>

<p>The following function can be used to make in inference for each 32 subsections of the <code class="language-plaintext highlighter-rouge">DecoderBlock</code>:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">inner</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">input_</span><span class="p">):</span>
    <span class="n">key</span><span class="p">,</span> <span class="n">seq</span> <span class="o">=</span> <span class="n">state</span>
    <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="nf">split_key_nullable</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="n">seq</span> <span class="o">=</span> <span class="nf">decoder_block</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">subkey</span><span class="p">,</span> <span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">)</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">seq</span><span class="p">),</span> <span class="bp">None</span>
<span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">seq</span><span class="p">),</span> <span class="n">_</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">scan</span><span class="p">(</span><span class="n">inner</span><span class="p">,</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">seq</span><span class="p">),</span> <span class="n">params</span><span class="p">)</span>
</code></pre></div></div>
<p>Inside the <code class="language-plaintext highlighter-rouge">inner</code> function, <code class="language-plaintext highlighter-rouge">input_</code> is represented in the following format:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DecoderBlock(
  input_norm=(4096),
  attention=Attention(
    q_proj=(4096, 1, 32, 128),
    k_proj=(4096, 32, 128),
    v_proj=(4096, 32, 128),
    out_proj=(1, 32, 128, 4096)
  ),
  post_attn_norm=(4096), gate_proj=(4096, 11008),
  up_proj=(4096, 11008), down_proj=(11008, 4096)
)
</code></pre></div></div>]]></content><author><name>Anique Tahir</name></author><summary type="html"><![CDATA[jax.lax.scan is a function which allows jit-able loops]]></summary></entry><entry><title type="html">New Blog</title><link href="https://anique.ml/blog/2022/website/" rel="alternate" type="text/html" title="New Blog" /><published>2022-08-06T00:00:00+00:00</published><updated>2022-08-06T00:00:00+00:00</updated><id>https://anique.ml/blog/2022/website</id><content type="html" xml:base="https://anique.ml/blog/2022/website/"><![CDATA[<p>Since my focus has shifted, I have decided to put down the old website. The old blog mainly consisted of 
posts about software development and random things I was doing. I had not updated it in several years since
my focus had shifted more towards my PhD studies. This new website will follow suit.</p>

<p>For any information about the old content, please feel free to reach out to me.</p>]]></content><author><name></name></author><category term="website" /><category term="website" /><summary type="html"><![CDATA[hello]]></summary></entry></feed>
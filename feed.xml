<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://anique.ml/feed.xml" rel="self" type="application/atom+xml" /><link href="https://anique.ml/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-12-07T10:25:04+00:00</updated><id>https://anique.ml/feed.xml</id><title type="html">blank</title><subtitle>Anique&apos;s Website.
</subtitle><entry><title type="html">Llama-2 finetuning using PEFT and Multi-GPU setup</title><link href="https://anique.ml/blog/2023/jax_lora/" rel="alternate" type="text/html" title="Llama-2 finetuning using PEFT and Multi-GPU setup" /><published>2023-12-06T15:59:00+00:00</published><updated>2023-12-06T15:59:00+00:00</updated><id>https://anique.ml/blog/2023/jax_lora</id><content type="html" xml:base="https://anique.ml/blog/2023/jax_lora/"><![CDATA[<p>ChatGPT and Gemini are great models for general prompting. However, due to model alignment, they refuse to respond to prompts related to research which involves toxic language. Llama-2 provides an open-source alternative to train an unaligned model. Thus, for one of my recent research, we needed to fine-tune a Llama-2 model. For the larger models, I also needed multi-gpu setup to fit the model in memory for training, especially due to large context sizes. However, I soon realized that open-source implementations of LoRA training implementations are limited. Multi-GPU inference on the other hand is as simple as using <code class="language-plaintext highlighter-rouge">auto</code> for the device mapping in the hugging face implementation.</p>

<p>“There’s two strategies that have been shown to work: Gpipe-style model parallelism, and tensor parallelism. HF Accelerate and Deepspeed both support the former. However sadly they don’t properly support LoRA at present.” -<a href="https://www.reddit.com/r/LocalLLaMA/comments/166h6bx/model_parallelism_with_lora/jyk5q6j/">Jeremy Howard</a></p>

<p>In addition to the limitations of accelerate and deepspeed, I also tried #anyscale’s ray/alpa to further disappointment. Perhaps thing’s have changed recently since I started working on this. However, with my solution, I never needed to look back.</p>

<p>I have always found Jax to be convenient to work with and much faster than pytorch, so I implemented LoRA and GPU tensor parallelism on top of <a href="https://github.com/ayaka14732/llama-2-jax">ayaka14732</a>’s excellent implementation of Llama-2 on JAX. In addition, I change the codebase to implement the following changes:</p>
<ul>
  <li>Alpaca format instruct dataset loading</li>
  <li>Parameter configuration for 13B models</li>
  <li>Hand-tuned sharding for projection and attention parameters for optimized distribution of parameters across GPU’s</li>
  <li>Merging of parameters to huggingface after training (for using other huggingface compatible libraries)</li>
  <li>python 3.9 support</li>
</ul>

<p>Code for the implementation is available here:
https://github.com/aniquetahir/llama-2-jax</p>

<p>#machinelearning #generativeai</p>]]></content><author><name></name></author><category term="machine-learning" /><category term="jax" /><summary type="html"><![CDATA[repository for the implementation of PEFT finetuning using jax]]></summary></entry><entry><title type="html">training large language models using distributed resources</title><link href="https://anique.ml/blog/2023/distributed_llms/" rel="alternate" type="text/html" title="training large language models using distributed resources" /><published>2023-07-27T00:00:00+00:00</published><updated>2023-07-27T00:00:00+00:00</updated><id>https://anique.ml/blog/2023/distributed_llms</id><content type="html" xml:base="https://anique.ml/blog/2023/distributed_llms/"><![CDATA[<h2 id="why-do-we-need-distributed-training">Why do we need Distributed Training?</h2>
<p>Large language models such as OpenAI’s ChatGPT and Meta’s Llama have gained popularity due to their usefulness. One of the major criticism of ChatGPT is that it’s training parameters are ironically closed source. However, Meta’s Llama models provide an Open Source alternative albeit being behind in quality. The NLP community has leveraged Llama to create variations which are intended to improve on the original model. Some improvements include uncensoring the model so that it is capable of responding in an unmoderated manner or finetuning it to provide better responses to instruct prompts. The possibilities of using open source LLMs are limitless. One simple application is to fine-tune Llama on a specific dataset. However, due to the size of the model, some of the larger versions may not fit on a single GPU. Hence, it is vital to learn how to train these models over a number of GPUs (multi-gpu) or a number of nodes each consisting of several GPUs (multi-node).</p>

<h2 id="basics-of-distributed-training">Basics of Distributed Training</h2>
<p>In this post, I will focus on distributed training over a single node containing multiple GPUs. One does not need to train models on a GPU. In fact, if one were interested in using the cloud, TPUs are a promising alternative. However, TPUs are not sold to the public and I am a promoted of doing things on hardware which one has physical access to. Thus, I will focus on training on NVidia GPUs (the other alternative, AMD, does not have good support for machine learning as per current date). This post will be hands-on with code examples you can run.</p>

<p>The first step is to check whether your rig has multiple GPUs and how they are linked together. The following command will return the status of the GPUs including the memory consumption and the processing power being consumed. It is handy to run this utility during training to get an idea of the resources being consumed (or not being consumed). In case the processing power is not 100%, it is indicative of a bottleneck in the training script which needs to be fixed.</p>
<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvidia-smi
</code></pre></div></div>

<p>Next, the following command helps to see how the GPUs are linked together:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>nvidia-smi topo -m
</code></pre></div></div>
<p>Output:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>	GPU0	GPU1	CPU Affinity	NUMA Affinity	GPU NUMA ID
GPU0	 X 	PHB	0-11		N/A		N/A
GPU1	PHB	 X 	0-11		N/A		N/A

Legend:

  X    = Self
  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)
  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node
  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)
  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)
  PIX  = Connection traversing at most a single PCIe bridge
  NV#  = Connection traversing a bonded set of # NVLinks
</code></pre></div></div>

<p>NVidia cards allow being linked together using NVlink cables. This helps cross-GPU communication. However, as visible from the output, the cards need not be linked in such a way. Even if the GPUs are connected to seperate PCI ports on the same node. They can still communicate. However, cross-GPU communication will be slower. When designing a model for distributed training, the topology of the resources needs to be taken into consideration.</p>

<p>There are multiple ways to parallelize a model e.g. Data Parallelism, Pipeline Parallelism, etc. There are pros and cons of each strategy and they can be mixed together.</p>

<h2 id="limitations-of-current-implementations">Limitations of current implementations</h2>

<hr />
<p>To be continued
(This post is a Work in Progress)</p>]]></content><author><name>Anique Tahir</name></author><summary type="html"><![CDATA[With the increasing popularity of large language models, it becomes increasingly important to use all available resources to gain an edge in training.]]></summary></entry><entry><title type="html">a post with giscus comments</title><link href="https://anique.ml/blog/2022/giscus-comments/" rel="alternate" type="text/html" title="a post with giscus comments" /><published>2022-12-10T15:59:00+00:00</published><updated>2022-12-10T15:59:00+00:00</updated><id>https://anique.ml/blog/2022/giscus-comments</id><content type="html" xml:base="https://anique.ml/blog/2022/giscus-comments/"><![CDATA[<p>This post shows how to add GISCUS comments.</p>]]></content><author><name></name></author><category term="sample-posts" /><category term="external-services" /><summary type="html"><![CDATA[an example of a blog post with giscus comments]]></summary></entry><entry><title type="html">understanding jax’s scan function</title><link href="https://anique.ml/blog/2022/understanding_scan/" rel="alternate" type="text/html" title="understanding jax’s scan function" /><published>2022-09-05T00:00:00+00:00</published><updated>2022-09-05T00:00:00+00:00</updated><id>https://anique.ml/blog/2022/understanding_scan</id><content type="html" xml:base="https://anique.ml/blog/2022/understanding_scan/"><![CDATA[<h2 id="why-use-jaxlaxscan">Why use <code class="language-plaintext highlighter-rouge">jax.lax.scan</code></h2>

<p>Jax is a neural network library used mostly by Google. Jax converts all your implementation into a graph which is executed
on your CPU, GPU or TPU. There are two main advantages of using Jax for your implementation:</p>
<ul>
  <li>It is comparatively faster than it’s competitor Pytorch</li>
  <li>It allows for significantly easier implementation for low level neural network concepts (albeit being harder for high-level</li>
  <li>ideas)</li>
</ul>

<p>Jax allows you to <code class="language-plaintext highlighter-rouge">jit</code> your functions. <code class="language-plaintext highlighter-rouge">jit</code> stands for Just-In-Time compilation. This makes your function significantly fast
since it is compiled into something native to the GPU<d-footnote>or any device you compile for.</d-footnote>. However, the drawback 
is that the amount of memory that your function will use, has to be pre-specified. This means that functions containing loops
have to be changed, since the length of the loop can be arbitrary. <code class="language-plaintext highlighter-rouge">jax.lax.scan</code> allows you to get around this limitation by
allowing you to define a loop with pre-specified length. But how does it work?</p>

<hr />

<p>According to the jax <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.lax.scan.html">documentation</a>, the following code is 
essentially a translation of the function in pythonic form:</p>

<pre><code class="language-python3">def scan(f, init, xs, length=None):
  if xs is None:
    xs = [None] * length
  carry = init
  ys = []
  for x in xs:
    carry, y = f(carry, x)
    ys.append(y)
  return carry, np.stack(ys)
</code></pre>

<p>The code may be a little convoluted to understand. A simpler way to understand it is to look at some simple examples. The <code class="language-plaintext highlighter-rouge">scan</code> 
function takes three parameters and scans over the third argument. The first arguments is a function to execute over each scan iteration.
The second argument is some <code class="language-plaintext highlighter-rouge">pytree</code> structure which we initially start from. Lets look at a simple example.</p>

<pre><code class="language-python3">scan(lambda x, y: (x+y, y+2,), 0, [1, 2, 3])
</code></pre>

<p>The above code essentially scans through the list <code class="language-plaintext highlighter-rouge">[1, 2, 3]</code> and for each element, returns a tuple consisting of the previous element and the current element. The output of the above code will be:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">])</span>
</code></pre></div></div>
<p>Here <code class="language-plaintext highlighter-rouge">x</code> is the carry argument which is initialized to <code class="language-plaintext highlighter-rouge">0</code>. <code class="language-plaintext highlighter-rouge">y</code> is each element of the array <code class="language-plaintext highlighter-rouge">[1, 2, 3]</code> passed sequentially to the function. In the first pass <code class="language-plaintext highlighter-rouge">x</code> is <code class="language-plaintext highlighter-rouge">0</code> and <code class="language-plaintext highlighter-rouge">y</code> is 1. The return value is <code class="language-plaintext highlighter-rouge">(x+y, y+2,) = (0+1,  1+2,) = (1, 3,)</code>. For the next iteration, <code class="language-plaintext highlighter-rouge">x</code> is <code class="language-plaintext highlighter-rouge">1</code>, since <code class="language-plaintext highlighter-rouge">1</code> is the value of the carry returned in the last iteration and <code class="language-plaintext highlighter-rouge">y</code> is <code class="language-plaintext highlighter-rouge">2</code>, since <code class="language-plaintext highlighter-rouge">2</code> is the next value in the input array. Thus, <code class="language-plaintext highlighter-rouge">(x+y, y+2,) = (1+2,  2+2,) = (3, 4,)</code>. In the next iteration, the final value of the carry is <code class="language-plaintext highlighter-rouge">6</code>, while the final <code class="language-plaintext highlighter-rouge">y+2</code> is <code class="language-plaintext highlighter-rouge">5</code>. Thus, the <code class="language-plaintext highlighter-rouge">scan</code> function returns <code class="language-plaintext highlighter-rouge">6</code>(the carry) and <code class="language-plaintext highlighter-rouge">[3, 4, 5]</code>(all the <code class="language-plaintext highlighter-rouge">y+2</code> concatenated)</p>

<p>Here we used a simple example, but the scan function can be used over more complicated arguments, such as the training loop of a neural network. This makes it possible to jit compile the entire training process, resulting in a large gain in training speed.</p>

<h2 id="useful-notes-on-scan-behaviour">Useful notes on scan behaviour</h2>
<p>In jax, it is common to use <code class="language-plaintext highlighter-rouge">NamedTuple</code>s to store various things such as model parameters. The third argument of scan can be used to iterate over the batch dimension of elements in the <code class="language-plaintext highlighter-rouge">NamedTuple</code>. Here is an example. Consider a transformer decoder with 32 blocks (Llama). Parameter values may be saved in a single <code class="language-plaintext highlighter-rouge">NamedTuple</code> as follows:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DecoderBlock(
  input_norm=(32, 4096),
  attention=Attention(
    q_proj=(32, 4096, 1, 32, 128),
    k_proj=(32, 4096, 32, 128),
    v_proj=(32, 4096, 32, 128),
    out_proj=(32, 1, 32, 128, 4096)
  ),
  post_attn_norm=(32, 4096), gate_proj=(32, 4096, 11008),
  up_proj=(32, 4096, 11008), down_proj=(32, 11008, 4096)
)
</code></pre></div></div>

<p>The following function can be used to make in inference for each 32 subsections of the <code class="language-plaintext highlighter-rouge">DecoderBlock</code>:</p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">inner</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">input_</span><span class="p">):</span>
    <span class="n">key</span><span class="p">,</span> <span class="n">seq</span> <span class="o">=</span> <span class="n">state</span>
    <span class="n">key</span><span class="p">,</span> <span class="n">subkey</span> <span class="o">=</span> <span class="nf">split_key_nullable</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>
    <span class="n">seq</span> <span class="o">=</span> <span class="nf">decoder_block</span><span class="p">(</span><span class="n">input_</span><span class="p">,</span> <span class="n">seq</span><span class="p">,</span> <span class="n">attn_mask</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">subkey</span><span class="p">,</span> <span class="n">model_config</span><span class="o">=</span><span class="n">model_config</span><span class="p">)</span>
    <span class="nf">return </span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">seq</span><span class="p">),</span> <span class="bp">None</span>
<span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">seq</span><span class="p">),</span> <span class="n">_</span> <span class="o">=</span> <span class="n">jax</span><span class="p">.</span><span class="n">lax</span><span class="p">.</span><span class="nf">scan</span><span class="p">(</span><span class="n">inner</span><span class="p">,</span> <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">seq</span><span class="p">),</span> <span class="n">params</span><span class="p">)</span>
</code></pre></div></div>
<p>Inside the <code class="language-plaintext highlighter-rouge">inner</code> function, <code class="language-plaintext highlighter-rouge">input_</code> is represented in the following format:</p>
<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>DecoderBlock(
  input_norm=(4096),
  attention=Attention(
    q_proj=(4096, 1, 32, 128),
    k_proj=(4096, 32, 128),
    v_proj=(4096, 32, 128),
    out_proj=(1, 32, 128, 4096)
  ),
  post_attn_norm=(4096), gate_proj=(4096, 11008),
  up_proj=(4096, 11008), down_proj=(11008, 4096)
)
</code></pre></div></div>]]></content><author><name>Anique Tahir</name></author><summary type="html"><![CDATA[jax.lax.scan is a function which allows jit-able loops]]></summary></entry><entry><title type="html">New Blog</title><link href="https://anique.ml/blog/2022/website/" rel="alternate" type="text/html" title="New Blog" /><published>2022-08-06T00:00:00+00:00</published><updated>2022-08-06T00:00:00+00:00</updated><id>https://anique.ml/blog/2022/website</id><content type="html" xml:base="https://anique.ml/blog/2022/website/"><![CDATA[<p>Since my focus has shifted, I have decided to put down the old website. The old blog mainly consisted of 
posts about software development and random things I was doing. I had not updated it in several years since
my focus had shifted more towards my PhD studies. This new website will follow suit.</p>

<p>For any information about the old content, please feel free to reach out to me.</p>]]></content><author><name></name></author><category term="website" /><category term="website" /><summary type="html"><![CDATA[hello]]></summary></entry></feed>